{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMtpJhxJtqv9",
    "outputId": "7e1d7fed-dc8c-4ea5-a77c-04dc52e54528"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SIGNIFICANT WORDS FOR EACH TOPIC:\n",
      "Topic 0: cell, theory, conference, algebra, understanding, quantum, teaching, fadd, faculty, learning\n",
      "Topic 1: theory, problem, analysis, equation, operator, mathematical, pi, faculty, one, group\n",
      "Topic 2: material, education, technology, industry, biotechnology, data, field, model, high, technician\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import google\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Due to the nature of Latent Dirichlet Allocation (LDA), I am setting a seed so that results can be repplicated\n",
    "np.random.seed(1998)\n",
    "\n",
    "#Due to computing capabilities, I am retrieving 100 documents, which are in xml format inside abstracts/\n",
    "folder_path = \"abstracts/\"\n",
    "documents = []\n",
    "file_names = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        abstract_narration = root.find(\"./Award/AbstractNarration\")\n",
    "        if abstract_narration is not None and abstract_narration.text:\n",
    "            text = abstract_narration.text.strip()\n",
    "            documents.append(text)\n",
    "            file_names.append(filename)\n",
    "########################\n",
    "\n",
    "#Cleaning the data of common phrases in documents\n",
    "documents = [str(document).replace('award reflects NSF\\'s statutory mission and has been deemed worthy of support through evaluation using the Foundation\\'s intellectual merit and broader impacts review criteria','') for document in documents]\n",
    "documents = [str(document).replace('Nontechnical Summary:','') for document in documents]\n",
    "documents = [str(document).replace('Technical summary:','') for document in documents]\n",
    "documents = [str(document).replace('Nontechnical Description','') for document in documents]\n",
    "documents = [str(document).replace('&lt;br/&gt;&lt;br/&gt','') for document in documents]\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#I am setting a new set of stopwrods, which are based on the scenario given (NSF Research Awards Abstracts)\n",
    "#This prevents ending up with topics that contain these words, which are commons accross most documents\n",
    "stop_words_scenario = ['nsf','undergraduate','goal','graduate','develop','development','new','provide','provided','stem','study','system','systems','college','program','science','research','project','student','using','support','award','impact','university','students','impacts','also','researcher','researchers']\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenizing\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # Removing stop words and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in stop_words_scenario]\n",
    "    return tokens\n",
    "\n",
    "# Tokenizing the documents\n",
    "tokenized_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Creating a dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "\n",
    "# Creating a document-term matrix using doc to bag of words\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "# Performing topic modeling using Latent Dirichlet Allocation (LDA)\n",
    "lda_model = models.LdaModel(doc_term_matrix, num_topics=3, id2word=dictionary, passes=20)\n",
    "\n",
    "# Extracting the topic probabilities for each document\n",
    "topic_probabilities = [lda_model.get_document_topics(doc) for doc in doc_term_matrix]\n",
    "\n",
    "# Converting the topic probabilities into a feature matrix\n",
    "feature_matrix = np.zeros((len(documents), lda_model.num_topics))\n",
    "for i, doc_topics in enumerate(topic_probabilities):\n",
    "    for topic, prob in doc_topics:\n",
    "        feature_matrix[i, topic] = prob\n",
    "        \n",
    "# Printing the 5 most common words associated with each topic\n",
    "print(\"MOST SIGNIFICANT WORDS FOR EACH TOPIC:\")\n",
    "topics = lda_model.print_topics(num_topics=lda_model.num_topics, num_words=10)  # Adjust the number of words as needed\n",
    "for topic in topics:\n",
    "    topic_id, topic_words = topic\n",
    "    topic_words = topic_words.split(\"+\")\n",
    "    topic_words = [word.split(\"*\")[1].strip().replace('\"', '') for word in topic_words]\n",
    "    topic_words_string = \", \".join(topic_words)\n",
    "    print(f\"Topic {topic_id}: {topic_words_string}\")\n",
    "        \n",
    "# Useing k-means clustering for document classification\n",
    "num_clusters = 3  \n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(feature_matrix)\n",
    "\n",
    "# Exporting a csv file containing:\n",
    "# Name of document, cluster number, abstract\n",
    "classification_nsf = pd.DataFrame(columns=['file_name','cluster_number','abstract'])\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    classification_nsf.loc[len(classification_nsf)] = [file_names[i],str(label),documents[i]]\n",
    "    \n",
    "\n",
    "classification_nsf.to_csv('results.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7e-rGPZyOF7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
